{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3595a828-cd54-4431-a5c8-321edaf8da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD001 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — LightGBM]\n",
      "  R2: 0.9894 | MSE: 48.2539 | MAE: 5.0370 | RUL Score: 3236.1059\n",
      "[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\n",
      "Results :\n",
      "  R2 (mean±std):        -0.3631 ± 0.3911\n",
      "  MSE (mean±std):       4995.0485 ± 2816.6261\n",
      "  MAE (mean±std):       57.4197 ± 15.1311\n",
      "  RUL Score (mean±std): 125399549003.4736 ± 1207679375425.8418\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\n",
      "Embargo=0 Results:\n",
      "  R2 (mean±std):        -13.4234 ± 8.0440\n",
      "  MSE (mean±std):       8221.2526 ± 6482.7785\n",
      "  MAE (mean±std):       80.5886 ± 31.0111\n",
      "  RUL Score (mean±std): 2669292984012.6484 ± 45172488807766.0547\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results:\n",
      "  R2 (mean±std):        -27.5141 ± 33.1033\n",
      "  MSE (mean±std):       8857.2287 ± 6713.8486\n",
      "  MAE (mean±std):       85.0832 ± 31.1085\n",
      "  RUL Score (mean±std): 2669292976232.7139 ± 45172488807708.3672\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD001\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"lgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (as requested)\n",
    "LGB_PARAMS = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=-1,      # silence LightGBM warnings\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# LightGBM model builder\n",
    "# ============================================\n",
    "def build_lgb():\n",
    "    return lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline)\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "lgb_hold = build_lgb().fit(X_tr, y_tr)\n",
    "p_hold = lgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10)\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries)\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — LightGBM]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — LightGBM (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results :\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results:\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results:\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6d510f-bca5-4ad9-a45c-4b61c0f2b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD002 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — LightGBM]\n",
      "  R2: 0.9832 | MSE: 79.0024 | MAE: 6.7962 | RUL Score: 13277.0178\n",
      "[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\n",
      "Results :\n",
      "  R2 (mean±std):        -0.4369 ± 0.4464\n",
      "  MSE (mean±std):       5347.5968 ± 3214.0103\n",
      "  MAE (mean±std):       58.9451 ± 16.3755\n",
      "  RUL Score (mean±std): 13576101086265.7461 ± 345108954178335.0625\n",
      "  Folds:                1040\n",
      "\n",
      "[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\n",
      "Embargo=0 Results:\n",
      "  R2 (mean±std):        -23.4150 ± 20.5719\n",
      "  MSE (mean±std):       10683.7284 ± 6361.2987\n",
      "  MAE (mean±std):       94.1182 ± 27.6772\n",
      "  RUL Score (mean±std): 17857124866170.3828 ± 457822180716058.5000\n",
      "  Folds:                1040\n",
      "\n",
      "Embargo=10 Results:\n",
      "  R2 (mean±std):        -48.4105 ± 68.1574\n",
      "  MSE (mean±std):       11443.1721 ± 6588.4857\n",
      "  MAE (mean±std):       98.5619 ± 28.0586\n",
      "  RUL Score (mean±std): 17857122986854.3008 ± 457822180787785.7500\n",
      "  Folds:                1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD002\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"lgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (as requested)\n",
    "LGB_PARAMS = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=-1,      # silence LightGBM warnings\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# LightGBM model builder\n",
    "# ============================================\n",
    "def build_lgb():\n",
    "    return lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline)\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "lgb_hold = build_lgb().fit(X_tr, y_tr)\n",
    "p_hold = lgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10)\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries)\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — LightGBM]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — LightGBM (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results :\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results:\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results:\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5febdafe-a8d7-42e4-bfe4-335ceb79154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD003 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — LightGBM]\n",
      "  R2: 0.9898 | MSE: 99.7244 | MAE: 6.9449 | RUL Score: 15041.6702\n",
      "[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\n",
      "Results :\n",
      "  R2 (mean±std):        -0.3849 ± 0.4397\n",
      "  MSE (mean±std):       7803.1683 ± 6243.0655\n",
      "  MAE (mean±std):       69.7110 ± 25.2408\n",
      "  RUL Score (mean±std): 43056464140658896.0000 ± 610550778194821632.0000\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\n",
      "Embargo=0 Results:\n",
      "  R2 (mean±std):        -12.2814 ± 6.5595\n",
      "  MSE (mean±std):       12152.0036 ± 12669.2231\n",
      "  MAE (mean±std):       94.5143 ± 43.5410\n",
      "  RUL Score (mean±std): 9771996031213809664.0000 ± 177496650878245535744.0000\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results:\n",
      "  R2 (mean±std):        -22.4704 ± 21.9772\n",
      "  MSE (mean±std):       12907.0482 ± 12988.0567\n",
      "  MAE (mean±std):       99.1431 ± 43.6701\n",
      "  RUL Score (mean±std): 9771996031213740032.0000 ± 177496650878245535744.0000\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD003\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"lgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (as requested)\n",
    "LGB_PARAMS = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=-1,      # silence LightGBM warnings\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# LightGBM model builder\n",
    "# ============================================\n",
    "def build_lgb():\n",
    "    return lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline)\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "lgb_hold = build_lgb().fit(X_tr, y_tr)\n",
    "p_hold = lgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10)\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries)\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — LightGBM]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — LightGBM (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results :\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results:\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results:\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01486a8-024e-4bdb-8758-82ab2a44baed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD004 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — LightGBM]\n",
      "  R2: 0.9830 | MSE: 136.9355 | MAE: 8.8780 | RUL Score: 25974.5151\n",
      "[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\n",
      "Results :\n",
      "  R2 (mean±std):        -0.4305 ± 0.3565\n",
      "  MSE (mean±std):       7874.3636 ± 5559.4215\n",
      "  MAE (mean±std):       70.2189 ± 23.2377\n",
      "  RUL Score (mean±std): 540974661719257728.0000 ± 12406418392449462272.0000\n",
      "  Folds:                996\n",
      "\n",
      "[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\n",
      "Embargo=0 Results:\n",
      "  R2 (mean±std):        -19.4229 ± 16.6162\n",
      "  MSE (mean±std):       14481.7819 ± 11311.7217\n",
      "  MAE (mean±std):       106.6296 ± 39.0857\n",
      "  RUL Score (mean±std): 25807753653300912128.0000 ± 558482759989211430912.0000\n",
      "  Folds:                996\n",
      "\n",
      "Embargo=10 Results:\n",
      "  R2 (mean±std):        -36.8880 ± 53.0221\n",
      "  MSE (mean±std):       15354.4717 ± 11603.3629\n",
      "  MAE (mean±std):       111.2491 ± 39.2876\n",
      "  RUL Score (mean±std): 25807753652877291520.0000 ± 558482759989230370816.0000\n",
      "  Folds:                996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD004\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"lgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (as requested)\n",
    "LGB_PARAMS = dict(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=-1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=-1,      # silence LightGBM warnings\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import clone\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# LightGBM model builder\n",
    "# ============================================\n",
    "def build_lgb():\n",
    "    return lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline)\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "lgb_hold = build_lgb().fit(X_tr, y_tr)\n",
    "p_hold = lgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10)\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_lgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_LGB\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries)\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — LightGBM]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — LightGBM (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) - LightGBM, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results :\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] - LightGBM (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results:\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results:\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d093532-9f64-4b4a-b1d6-815ceeeb1ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD001 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — CatBoost]\n",
      "  R2: 0.9872 | MSE: 58.3519 | MAE: 5.8927 | RUL Score: 3761.2999\n",
      "[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.1523 ± 0.1833\n",
      "  MSE (mean±std):       4244.0547 ± 2322.6343\n",
      "  MAE (mean±std):       54.0617 ± 13.5151\n",
      "  RUL Score (mean±std): 3882929923.2324 ± 44450061816.1704\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -19.6719 ± 14.2012\n",
      "  MSE (mean±std):       9950.8117 ± 6364.9369\n",
      "  MAE (mean±std):       91.3019 ± 27.6341\n",
      "  RUL Score (mean±std): 3092642413689.4863 ± 49818096206064.9766\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -41.4518 ± 54.6701\n",
      "  MSE (mean±std):       10765.0992 ± 6539.1773\n",
      "  MAE (mean±std):       96.4234 ± 27.4402\n",
      "  RUL Score (mean±std): 3092642409465.3413 ± 49818096205496.8984\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD001\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"catboost_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (CatBoost)\n",
    "CB_PARAMS = dict(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=0,         # silence CatBoost training logs\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# CatBoost model builder\n",
    "# ============================================\n",
    "def build_cb():\n",
    "    return CatBoostRegressor(**CB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — CatBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "cb_hold = build_cb().fit(X_tr, y_tr)\n",
    "p_hold = cb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_CatBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — CatBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — CatBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — CatBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — CatBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — CatBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — CatBoost (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_CatBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23b0f2e-5f89-4ff8-813e-f0b6c2f1bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD002 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — CatBoost]\n",
      "  R2: 0.9498 | MSE: 235.6130 | MAE: 12.0735 | RUL Score: 39339.4042\n",
      "[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.3717 ± 0.2716\n",
      "  MSE (mean±std):       5028.9107 ± 2620.1954\n",
      "  MAE (mean±std):       57.6963 ± 14.2238\n",
      "  RUL Score (mean±std): 326837721045.9213 ± 9513441848210.2676\n",
      "  Folds:                1040\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -22.7586 ± 18.4219\n",
      "  MSE (mean±std):       10625.4708 ± 6343.3908\n",
      "  MAE (mean±std):       94.8369 ± 26.6789\n",
      "  RUL Score (mean±std): 15485141355664.2148 ± 348992854105390.0625\n",
      "  Folds:                1040\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -47.1226 ± 62.5902\n",
      "  MSE (mean±std):       11420.2588 ± 6533.8169\n",
      "  MAE (mean±std):       99.5583 ± 26.7327\n",
      "  RUL Score (mean±std): 15485141329657.5859 ± 348992854105941.9375\n",
      "  Folds:                1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD002\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"catboost_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (CatBoost)\n",
    "CB_PARAMS = dict(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=0,         # silence CatBoost training logs\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# CatBoost model builder\n",
    "# ============================================\n",
    "def build_cb():\n",
    "    return CatBoostRegressor(**CB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — CatBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "cb_hold = build_cb().fit(X_tr, y_tr)\n",
    "p_hold = cb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_CatBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — CatBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — CatBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — CatBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — CatBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — CatBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — CatBoost (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_CatBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c8ebf7-2b01-4acc-b128-8cb3bbf30743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD003 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — CatBoost]\n",
      "  R2: 0.9854 | MSE: 143.2805 | MAE: 8.6236 | RUL Score: 21036.0152\n",
      "[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.1694 ± 0.2336\n",
      "  MSE (mean±std):       6644.2492 ± 5324.2171\n",
      "  MAE (mean±std):       65.5176 ± 23.4308\n",
      "  RUL Score (mean±std): 48685974422202.9766 ± 617439568807418.0000\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -16.8319 ± 11.9293\n",
      "  MSE (mean±std):       13920.0070 ± 12967.4665\n",
      "  MAE (mean±std):       103.7241 ± 41.2806\n",
      "  RUL Score (mean±std): 19091951148569608192.0000 ± 368981738485275164672.0000\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -32.1086 ± 40.2939\n",
      "  MSE (mean±std):       14835.3068 ± 13253.6739\n",
      "  MAE (mean±std):       108.9007 ± 41.1822\n",
      "  RUL Score (mean±std): 19091951148569591808.0000 ± 368981738485275099136.0000\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD003\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"catboost_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (CatBoost)\n",
    "CB_PARAMS = dict(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=0,         # silence CatBoost training logs\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# CatBoost model builder\n",
    "# ============================================\n",
    "def build_cb():\n",
    "    return CatBoostRegressor(**CB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — CatBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "cb_hold = build_cb().fit(X_tr, y_tr)\n",
    "p_hold = cb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_CatBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — CatBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — CatBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — CatBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — CatBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — CatBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — CatBoost (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_CatBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bb73ad-e45e-4d36-bc2a-566b190305c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD004 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — CatBoost]\n",
      "  R2: 0.9403 | MSE: 479.8289 | MAE: 16.9850 | RUL Score: 174405.5979\n",
      "[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.3537 ± 0.2555\n",
      "  MSE (mean±std):       7196.4212 ± 4513.8248\n",
      "  MAE (mean±std):       68.1682 ± 20.4050\n",
      "  RUL Score (mean±std): 239274966969255.1875 ± 4103275332667334.0000\n",
      "  Folds:                996\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -19.1938 ± 15.4521\n",
      "  MSE (mean±std):       14408.4218 ± 11265.4815\n",
      "  MAE (mean±std):       107.3575 ± 38.1147\n",
      "  RUL Score (mean±std): 36269861794884620288.0000 ± 1115284983045348524032.0000\n",
      "  Folds:                996\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -36.7452 ± 51.3636\n",
      "  MSE (mean±std):       15311.1655 ± 11532.9710\n",
      "  MAE (mean±std):       112.2038 ± 38.0998\n",
      "  RUL Score (mean±std): 36269861794762805248.0000 ± 1115284983045352718336.0000\n",
      "  Folds:                996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD004\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"catboost_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (CatBoost)\n",
    "CB_PARAMS = dict(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=0,         # silence CatBoost training logs\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score is renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# CatBoost model builder\n",
    "# ============================================\n",
    "def build_cb():\n",
    "    return CatBoostRegressor(**CB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — CatBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "cb_hold = build_cb().fit(X_tr, y_tr)\n",
    "p_hold = cb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_CatBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — CatBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — CatBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_cb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_CatBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — CatBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — CatBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — CatBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main\n",
    "#report_lines.append(\"\\n[PGTS — CatBoost (Embargo=10)]\")\n",
    "#report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — CatBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — CatBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_CatBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fc8451-74b3-466c-ad5f-5531019dcbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD001 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — GradientBoosting]\n",
      "  R2: 0.9840 | MSE: 72.9487 | MAE: 6.0383 | RUL Score: 5748.0022\n",
      "[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.2952 ± 0.2805\n",
      "  MSE (mean±std):       4697.7835 ± 2471.0137\n",
      "  MAE (mean±std):       56.1931 ± 13.7121\n",
      "  RUL Score (mean±std): 2754605875.8714 ± 29687775830.3209\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -16.8471 ± 10.6445\n",
      "  MSE (mean±std):       9275.6666 ± 6472.0449\n",
      "  MAE (mean±std):       87.2176 ± 28.9685\n",
      "  RUL Score (mean±std): 3398934526074.6626 ± 57268855973195.1016\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -34.6057 ± 41.9510\n",
      "  MSE (mean±std):       10018.2798 ± 6680.5703\n",
      "  MAE (mean±std):       92.0705 ± 28.9645\n",
      "  RUL Score (mean±std): 3398934519530.2124 ± 57268855970084.9688\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD001\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"gb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (Gradient Boosting)\n",
    "GB_PARAMS = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# Gradient Boosting model builder\n",
    "# ============================================\n",
    "def build_gb():\n",
    "    return GradientBoostingRegressor(**GB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — Gradient Boosting\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "gb_hold = build_gb().fit(X_tr, y_tr)\n",
    "p_hold = gb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_GradientBoosting.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — Gradient Boosting\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — Gradient Boosting\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — GradientBoosting]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (aynı CatBoost örneğinde olduğu gibi satırı saklı tutabilir veya açabilirsin)\n",
    "# report_lines.append(\"\\n[PGTS — GradientBoosting (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_GradientBoosting.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb20495-4737-4daf-b0c4-f93ea3819c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD002 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — GradientBoosting]\n",
      "  R2: 0.9804 | MSE: 91.7819 | MAE: 7.3138 | RUL Score: 15072.2448\n",
      "[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.4315 ± 0.2857\n",
      "  MSE (mean±std):       5245.0847 ± 2734.3053\n",
      "  MAE (mean±std):       58.7280 ± 14.5352\n",
      "  RUL Score (mean±std): 129150615916.7789 ± 1918472227342.8181\n",
      "  Folds:                1040\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -22.9671 ± 18.7117\n",
      "  MSE (mean±std):       10644.0295 ± 6321.2825\n",
      "  MAE (mean±std):       95.0231 ± 26.4396\n",
      "  RUL Score (mean±std): 12433577496370.4980 ± 247596363216614.0312\n",
      "  Folds:                1040\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -47.5316 ± 62.9211\n",
      "  MSE (mean±std):       11431.9680 ± 6516.2747\n",
      "  MAE (mean±std):       99.6711 ± 26.5559\n",
      "  RUL Score (mean±std): 12433577461629.1875 ± 247596363217605.1250\n",
      "  Folds:                1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD002\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"gb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (Gradient Boosting)\n",
    "GB_PARAMS = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# Gradient Boosting model builder\n",
    "# ============================================\n",
    "def build_gb():\n",
    "    return GradientBoostingRegressor(**GB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — Gradient Boosting\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "gb_hold = build_gb().fit(X_tr, y_tr)\n",
    "p_hold = gb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_GradientBoosting.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — Gradient Boosting\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — Gradient Boosting\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — GradientBoosting]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (aynı CatBoost örneğinde olduğu gibi satırı saklı tutabilir veya açabilirsin)\n",
    "# report_lines.append(\"\\n[PGTS — GradientBoosting (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_GradientBoosting.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6fb8d74-6bda-444e-ad3a-ea8c8f02ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD003 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — GradientBoosting]\n",
      "  R2: 0.9866 | MSE: 130.9718 | MAE: 7.8019 | RUL Score: 28202.9870\n",
      "[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.3164 ± 0.3123\n",
      "  MSE (mean±std):       7322.5549 ± 5771.3948\n",
      "  MAE (mean±std):       68.1245 ± 23.5971\n",
      "  RUL Score (mean±std): 2176135449317262.7500 ± 31034646256604252.0000\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -14.6885 ± 9.2831\n",
      "  MSE (mean±std):       13292.9226 ± 13399.2947\n",
      "  MAE (mean±std):       99.9149 ± 43.2716\n",
      "  RUL Score (mean±std): 55770330263616946176.0000 ± 1077222843725632241664.0000\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -27.5954 ± 31.7693\n",
      "  MSE (mean±std):       14147.5856 ± 13724.4535\n",
      "  MAE (mean±std):       104.9020 ± 43.3237\n",
      "  RUL Score (mean±std): 55770330263616937984.0000 ± 1077222843725632241664.0000\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD003\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"gb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (Gradient Boosting)\n",
    "GB_PARAMS = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# Gradient Boosting model builder\n",
    "# ============================================\n",
    "def build_gb():\n",
    "    return GradientBoostingRegressor(**GB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — Gradient Boosting\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "gb_hold = build_gb().fit(X_tr, y_tr)\n",
    "p_hold = gb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_GradientBoosting.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — Gradient Boosting\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — Gradient Boosting\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — GradientBoosting]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (aynı CatBoost örneğinde olduğu gibi satırı saklı tutabilir veya açabilirsin)\n",
    "# report_lines.append(\"\\n[PGTS — GradientBoosting (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_GradientBoosting.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f44f48-1a59-4873-a16b-5fc71116a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD004 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — GradientBoosting]\n",
      "  R2: 0.9779 | MSE: 177.9456 | MAE: 10.0170 | RUL Score: 38718.4295\n",
      "[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.4035 ± 0.2781\n",
      "  MSE (mean±std):       7455.2237 ± 4709.8973\n",
      "  MAE (mean±std):       69.1501 ± 20.7402\n",
      "  RUL Score (mean±std): 5383978822758562.0000 ± 85157925457180176.0000\n",
      "  Folds:                996\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -19.6587 ± 15.3985\n",
      "  MSE (mean±std):       14666.2917 ± 11303.1595\n",
      "  MAE (mean±std):       108.2428 ± 38.1474\n",
      "  RUL Score (mean±std): 24660465452448976896.0000 ± 740371961084914040832.0000\n",
      "  Folds:                996\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -37.2196 ± 49.8725\n",
      "  MSE (mean±std):       15578.6291 ± 11576.4453\n",
      "  MAE (mean±std):       113.0612 ± 38.1911\n",
      "  RUL Score (mean±std): 24660465452370563072.0000 ± 740371961084916531200.0000\n",
      "  Folds:                996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD004\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"gb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (Gradient Boosting)\n",
    "GB_PARAMS = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# Gradient Boosting model builder\n",
    "# ============================================\n",
    "def build_gb():\n",
    "    return GradientBoostingRegressor(**GB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — Gradient Boosting\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "gb_hold = build_gb().fit(X_tr, y_tr)\n",
    "p_hold = gb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_GradientBoosting.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — Gradient Boosting\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_gb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_GradientBoosting\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — Gradient Boosting\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — Gradient Boosting\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — GradientBoosting]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (aynı CatBoost örneğinde olduğu gibi satırı saklı tutabilir veya açabilirsin)\n",
    "# report_lines.append(\"\\n[PGTS — GradientBoosting (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — GradientBoosting, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — GradientBoosting (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_GradientBoosting.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "390719b3-b7ec-4a1d-9e66-7f5ee9638c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD001 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — XGBoost]\n",
      "  R2: 0.9825 | MSE: 79.7548 | MAE: 6.8220 | RUL Score: 5054.8245\n",
      "[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.3206 ± 0.3580\n",
      "  MSE (mean±std):       4833.7805 ± 2857.8303\n",
      "  MAE (mean±std):       56.6898 ± 14.7574\n",
      "  RUL Score (mean±std): 9891627813.1592 ± 91491181841.5948\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -18.3422 ± 13.5650\n",
      "  MSE (mean±std):       9483.9062 ± 6340.7298\n",
      "  MAE (mean±std):       88.6217 ± 27.9377\n",
      "  RUL Score (mean±std): 3915047670640.4146 ± 60718162496540.4219\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -38.6473 ± 52.8056\n",
      "  MSE (mean±std):       10254.4031 ± 6530.3002\n",
      "  MAE (mean±std):       93.5969 ± 27.8659\n",
      "  RUL Score (mean±std): 3915047662011.1392 ± 60718162492217.1406\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD001\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"xgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (XGBoost)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    learning_rate=0.2,\n",
    "    max_depth=4,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# XGBoost model builder\n",
    "# ============================================\n",
    "def build_xgb():\n",
    "    return XGBRegressor(**XGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — XGBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "xgb_hold = build_xgb().fit(X_tr, y_tr)\n",
    "p_hold = xgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_XGBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — XGBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — XGBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — XGBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — XGBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — XGBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (opsiyonel olarak açılabilir)\n",
    "# report_lines.append(\"\\n[PGTS — XGBoost (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_XGBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac3aa24-7141-4e13-b6f8-e1703f0262da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD002 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — XGBoost]\n",
      "  R2: 0.9335 | MSE: 312.2766 | MAE: 13.0481 | RUL Score: 335032.3025\n",
      "[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.4679 ± 0.3038\n",
      "  MSE (mean±std):       5386.7397 ± 2830.7087\n",
      "  MAE (mean±std):       59.3320 ± 14.8057\n",
      "  RUL Score (mean±std): 100880657707.9924 ± 1205031470677.7039\n",
      "  Folds:                1040\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -22.3101 ± 18.3138\n",
      "  MSE (mean±std):       10551.2594 ± 6409.4406\n",
      "  MAE (mean±std):       94.1430 ± 27.3394\n",
      "  RUL Score (mean±std): 15857044393911.6934 ± 350626805314865.0000\n",
      "  Folds:                1040\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -46.0864 ± 62.9112\n",
      "  MSE (mean±std):       11320.8032 ± 6621.3009\n",
      "  MAE (mean±std):       98.6934 ± 27.5628\n",
      "  RUL Score (mean±std): 15857044360870.1699 ± 350626805315678.6250\n",
      "  Folds:                1040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD002\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"xgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (XGBoost)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    learning_rate=0.2,\n",
    "    max_depth=4,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# XGBoost model builder\n",
    "# ============================================\n",
    "def build_xgb():\n",
    "    return XGBRegressor(**XGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — XGBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "xgb_hold = build_xgb().fit(X_tr, y_tr)\n",
    "p_hold = xgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_XGBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — XGBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — XGBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — XGBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — XGBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — XGBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (opsiyonel olarak açılabilir)\n",
    "# report_lines.append(\"\\n[PGTS — XGBoost (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_XGBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f6e818f-58da-40ed-8dd7-493539341721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD003 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — XGBoost]\n",
      "  R2: 0.9779 | MSE: 216.6679 | MAE: 10.6474 | RUL Score: 27351.4890\n",
      "[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.3469 ± 0.3726\n",
      "  MSE (mean±std):       7641.5246 ± 6298.4434\n",
      "  MAE (mean±std):       68.9596 ± 25.0980\n",
      "  RUL Score (mean±std): 63871589225696440.0000 ± 1009944183417075712.0000\n",
      "  Folds:                400\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -15.3395 ± 9.6180\n",
      "  MSE (mean±std):       13492.5845 ± 13000.7709\n",
      "  MAE (mean±std):       101.1391 ± 42.4413\n",
      "  RUL Score (mean±std): 12396752775116976128.0000 ± 231553471733017280512.0000\n",
      "  Folds:                400\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -28.7058 ± 31.4559\n",
      "  MSE (mean±std):       14370.5464 ± 13309.0770\n",
      "  MAE (mean±std):       106.2103 ± 42.4538\n",
      "  RUL Score (mean±std): 12396752775116957696.0000 ± 231553471733017280512.0000\n",
      "  Folds:                400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD003\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"xgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (XGBoost)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    learning_rate=0.2,\n",
    "    max_depth=4,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# XGBoost model builder\n",
    "# ============================================\n",
    "def build_xgb():\n",
    "    return XGBRegressor(**XGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — XGBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "xgb_hold = build_xgb().fit(X_tr, y_tr)\n",
    "p_hold = xgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_XGBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — XGBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — XGBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — XGBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — XGBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — XGBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (opsiyonel olarak açılabilir)\n",
    "# report_lines.append(\"\\n[PGTS — XGBoost (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_XGBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc64f13e-9c58-4ee1-b114-d8fdbabd7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=FD004 | Feature count=24\n",
      "Purged Group Time Series Split (PGTS): Window(W)=30, Horizon(H)=1, Embargo=10, Splits=5\n",
      "\n",
      "[Random 80/20 Split — XGBoost]\n",
      "  R2: 0.9449 | MSE: 442.6934 | MAE: 16.1760 | RUL Score: 145513.5241\n",
      "[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\n",
      "Results\n",
      "  R2 (mean±std):        -0.4550 ± 0.2849\n",
      "  MSE (mean±std):       7761.1566 ± 5003.1169\n",
      "  MAE (mean±std):       70.2308 ± 21.2966\n",
      "  RUL Score (mean±std): 42698316085900280.0000 ± 886666804491315840.0000\n",
      "  Folds:                996\n",
      "\n",
      "[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\n",
      "Embargo=0 Results\n",
      "  R2 (mean±std):        -19.2626 ± 15.1279\n",
      "  MSE (mean±std):       14566.3360 ± 11325.9022\n",
      "  MAE (mean±std):       107.4866 ± 38.6735\n",
      "  RUL Score (mean±std): 16163087539408936960.0000 ± 483784126309245648896.0000\n",
      "  Folds:                996\n",
      "\n",
      "Embargo=10 Results\n",
      "  R2 (mean±std):        -36.2865 ± 48.2203\n",
      "  MSE (mean±std):       15461.5895 ± 11610.9668\n",
      "  MAE (mean±std):       112.2268 ± 38.7928\n",
      "  RUL Score (mean±std): 16163087539166672896.0000 ± 483784126309253775360.0000\n",
      "  Folds:                996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration — choose CMAPSS subset once\n",
    "# ============================================\n",
    "# Set to \"FD001\", \"FD002\", \"FD003\", or \"FD004\"\n",
    "DATASET_NAME = \"FD004\"\n",
    "\n",
    "# Human-readable tag for the model family (used in output folder)\n",
    "MODEL_TAG = \"xgb_only_pgts\"\n",
    "\n",
    "# Fixed model hyperparameters (XGBoost)\n",
    "XGB_PARAMS = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    learning_rate=0.2,\n",
    "    max_depth=4,\n",
    "    n_estimators=200,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PGTS config\n",
    "W = 30        # Window length (cycles) ~ purge guidance\n",
    "H = 1         # Horizon (steps ahead)\n",
    "EMBARGO = 10  # cycles to skip after cut\n",
    "N_SPLITS = 5  # number of cuts per engine\n",
    "\n",
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================================\n",
    "# Output paths — single, unified directory\n",
    "# ============================================\n",
    "OUTPUT_DIR = Path(f\"{MODEL_TAG}_{DATASET_NAME}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_txt(filename: str, text: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_DIR / Path(filename).name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "def save_df_csv(df: pd.DataFrame, filename: str) -> None:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / Path(filename).name, index=False)\n",
    "\n",
    "# ============================================\n",
    "# I/O — paths derived from DATASET_NAME\n",
    "# ============================================\n",
    "# NASA CMAPSS public data\n",
    "train_path = f\"train_{DATASET_NAME}.txt\"\n",
    "test_path  = f\"test_{DATASET_NAME}.txt\"\n",
    "rul_path   = f\"RUL_{DATASET_NAME}.txt\"\n",
    "\n",
    "# ============================================\n",
    "# Load\n",
    "# ============================================\n",
    "train = pd.read_csv(train_path, sep=r'\\s+', header=None)\n",
    "test  = pd.read_csv(test_path,  sep=r'\\s+', header=None)\n",
    "y_test_file = pd.read_csv(rul_path, sep=r'\\s+', header=None)\n",
    "\n",
    "# ============================================\n",
    "# Columns\n",
    "# ============================================\n",
    "columns = [\n",
    "    'id','cycle','setting1','setting2','setting3',\n",
    "    's1','s2','s3','s4','s5','s6','s7','s8','s9',\n",
    "    's10','s11','s12','s13','s14','s15','s16',\n",
    "    's17','s18','s19','s20','s21'\n",
    "]\n",
    "train.columns = columns\n",
    "test.columns  = columns\n",
    "\n",
    "# ============================================\n",
    "# Sort & clean\n",
    "# ============================================\n",
    "train.sort_values(['id','cycle'], inplace=True)\n",
    "test.sort_values(['id','cycle'], inplace=True)\n",
    "y_test_file.dropna(axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Compute training RUL\n",
    "# ============================================\n",
    "max_cycle_train = train.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_train.columns = ['id','max_cycle']\n",
    "train = train.merge(max_cycle_train, on='id', how='left')\n",
    "train['RUL'] = train['max_cycle'] - train['cycle']\n",
    "train.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Normalize features (fit on train only)\n",
    "# ============================================\n",
    "features = train.columns.difference(['id','cycle','RUL'])\n",
    "assert len(features) == 24, f\"Expected 24 features, got {len(features)}\"\n",
    "scaler = MinMaxScaler()\n",
    "train_norm = pd.DataFrame(\n",
    "    scaler.fit_transform(train[features]),\n",
    "    columns=features, index=train.index\n",
    ")\n",
    "train = train[['id','cycle','RUL']].join(train_norm)\n",
    "\n",
    "# ============================================\n",
    "# Prepare test set (transform with train scaler)\n",
    "# ============================================\n",
    "test_norm = pd.DataFrame(\n",
    "    scaler.transform(test[features]),\n",
    "    columns=features, index=test.index\n",
    ")\n",
    "test = (\n",
    "    test[test.columns.difference(features)]\n",
    "    .join(test_norm)\n",
    "    .reindex(columns=test.columns)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Compute test RUL from provided horizons\n",
    "# ============================================\n",
    "max_cycle_test = test.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_test.columns = ['id','max_cycle']\n",
    "y_test_file.columns = ['collected_RUL']\n",
    "y_test_file['id'] = y_test_file.index + 1\n",
    "y_test_file['max_cycle'] = max_cycle_test['max_cycle'] + y_test_file['collected_RUL']\n",
    "y_test_file.drop('collected_RUL', axis=1, inplace=True)\n",
    "test = test.merge(y_test_file, on='id', how='left')\n",
    "test['RUL'] = test['max_cycle'] - test['cycle']\n",
    "test.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# ============================================\n",
    "# Helpers: metrics (RUL Score = renamed PHM08)\n",
    "# ============================================\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = np.asarray(y_pred) - np.asarray(y_true)\n",
    "    # same functional form as PHM08, but reported as \"RUL Score\"\n",
    "    return float(np.sum(np.where(d < 0, np.exp(-d/13) - 1, np.exp(d/10) - 1)))\n",
    "\n",
    "def metrics_dict(y_true, y_pred, prefix=\"\"):\n",
    "    return {\n",
    "        f\"{prefix}R2\": r2_score(y_true, y_pred),\n",
    "        f\"{prefix}MSE\": mean_squared_error(y_true, y_pred),\n",
    "        f\"{prefix}MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        f\"{prefix}RUL_Score\": rul_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def summarize_table(rows, label):\n",
    "    \"\"\"rows: list of dicts with keys R2, MSE, MAE, RUL_Score\"\"\"\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df, None\n",
    "    summary = {\n",
    "        \"R2_mean\": df[\"R2\"].mean(),   \"R2_std\": df[\"R2\"].std(),\n",
    "        \"MSE_mean\": df[\"MSE\"].mean(), \"MSE_std\": df[\"MSE\"].std(),\n",
    "        \"MAE_mean\": df[\"MAE\"].mean(), \"MAE_std\": df[\"MAE\"].std(),\n",
    "        \"RUL_Score_mean\": df[\"RUL_Score\"].mean(), \"RUL_Score_std\": df[\"RUL_Score\"].std(),\n",
    "        \"n_folds\": len(df)\n",
    "    }\n",
    "    save_df_csv(df, f\"{label}_per_fold_{DATASET_NAME}.csv\")\n",
    "    save_df_csv(pd.DataFrame([summary]), f\"{label}_summary_{DATASET_NAME}.csv\")\n",
    "    return df, summary\n",
    "\n",
    "# ============================================\n",
    "# XGBoost model builder\n",
    "# ============================================\n",
    "def build_xgb():\n",
    "    return XGBRegressor(**XGB_PARAMS)\n",
    "\n",
    "# ============================================\n",
    "# Purged Group Time Series Split (PGTS)\n",
    "# ============================================\n",
    "ENGINE_COL = \"id\"\n",
    "TIME_COL   = \"cycle\"\n",
    "TARGET_COL = \"RUL\"\n",
    "ALL_FEATURES = [c for c in train.columns if c not in [ENGINE_COL, TIME_COL, TARGET_COL]]\n",
    "\n",
    "def pgts_splits_for_engine(df_engine, n_splits=N_SPLITS, window=W, horizon=H, embargo=EMBARGO):\n",
    "    g = df_engine.sort_values(TIME_COL).reset_index()\n",
    "    T = len(g)\n",
    "    if T <= (window + horizon + 1):\n",
    "        return []\n",
    "    # choose cut points excluding edges\n",
    "    cuts = np.linspace(window + horizon, T - horizon, num=n_splits+1, dtype=int)[1:]\n",
    "    splits = []\n",
    "    for cut in cuts:\n",
    "        train_end  = max(window, cut - (window - 1))  # purge ~ W-1\n",
    "        test_start = min(T - horizon, cut + embargo)\n",
    "        if test_start <= train_end or test_start >= T - horizon:\n",
    "            continue\n",
    "        tr_idx = g.loc[:train_end-1, \"index\"].values\n",
    "        te_idx = g.loc[test_start:T - horizon - 1, \"index\"].values\n",
    "        if len(te_idx) == 0 or len(tr_idx) == 0:\n",
    "            continue\n",
    "        splits.append((tr_idx, te_idx))\n",
    "    return splits\n",
    "\n",
    "def run_pgts(df_all, features, embargo=EMBARGO, label=\"PGTS\"):\n",
    "    rows = []\n",
    "    for _, df_e in df_all.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_all.loc[tr_idx, features]; y_tr = df_all.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_all.loc[te_idx, features]; y_te = df_all.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "# ============================================\n",
    "# Random 80/20 holdout (baseline) — XGBoost\n",
    "# ============================================\n",
    "X = train.drop('RUL', axis=1)\n",
    "y = train['RUL']\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "xgb_hold = build_xgb().fit(X_tr, y_tr)\n",
    "p_hold = xgb_hold.predict(X_va)\n",
    "hold_summary = metrics_dict(y_va, p_hold)\n",
    "save_df_csv(pd.DataFrame([hold_summary]), f\"holdout_summary_{DATASET_NAME}_XGBoost.csv\")\n",
    "\n",
    "# ============================================\n",
    "# PGTS main (Embargo=10) — XGBoost\n",
    "# ============================================\n",
    "df_all = train[[ENGINE_COL, TIME_COL, TARGET_COL] + ALL_FEATURES]\n",
    "pgts_e10_summary = run_pgts(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_E10\")\n",
    "\n",
    "# ============================================\n",
    "# Null model (label permutation) under PGTS — XGBoost\n",
    "# ============================================\n",
    "def run_pgts_null(df_all, features, embargo=10, label=\"PGTS_NULL_E10\", seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    df_perm = df_all.copy()\n",
    "    # permute labels within each engine independently\n",
    "    df_perm[TARGET_COL] = (\n",
    "        df_perm.groupby(ENGINE_COL)[TARGET_COL]\n",
    "               .transform(lambda s: s.values[rng.permutation(len(s))])\n",
    "    )\n",
    "    rows = []\n",
    "    for _, df_e in df_perm.groupby(ENGINE_COL, sort=False):\n",
    "        for (tr_idx, te_idx) in pgts_splits_for_engine(df_e, embargo=embargo):\n",
    "            X_tr = df_perm.loc[tr_idx, features]; y_tr = df_perm.loc[tr_idx, TARGET_COL]\n",
    "            X_te = df_perm.loc[te_idx, features]; y_te = df_perm.loc[te_idx, TARGET_COL]\n",
    "            mdl = build_xgb().fit(X_tr, y_tr)\n",
    "            p = mdl.predict(X_te)\n",
    "            rows.append(dict(R2=r2_score(y_te, p),\n",
    "                             MSE=mean_squared_error(y_te, p),\n",
    "                             MAE=mean_absolute_error(y_te, p),\n",
    "                             RUL_Score=rul_score(y_te, p)))\n",
    "    _, summary = summarize_table(rows, f\"{label}_XGBoost\")\n",
    "    return summary\n",
    "\n",
    "pgts_null_summary = run_pgts_null(df_all, ALL_FEATURES, embargo=10, label=\"PGTS_NULL_E10\", seed=42)\n",
    "\n",
    "# ============================================\n",
    "# Embargo sensitivity (E=0 vs E=10) under PGTS — XGBoost\n",
    "# ============================================\n",
    "pgts_e0_summary  = run_pgts(df_all, ALL_FEATURES, embargo=0,  label=\"PGTS_E0\")\n",
    "pgts_e10_summary = pgts_e10_summary  # already computed above\n",
    "\n",
    "# ============================================\n",
    "# FINAL: compact report (print only summaries) — XGBoost\n",
    "# ============================================\n",
    "def fmt_summary(title, summary_dict):\n",
    "    if summary_dict is None:\n",
    "        return f\"{title}: no valid folds.\"\n",
    "    return (\n",
    "        f\"{title}\\n\"\n",
    "        f\"  R2 (mean±std):        {summary_dict['R2_mean']:.4f} ± {summary_dict['R2_std']:.4f}\\n\"\n",
    "        f\"  MSE (mean±std):       {summary_dict['MSE_mean']:.4f} ± {summary_dict['MSE_std']:.4f}\\n\"\n",
    "        f\"  MAE (mean±std):       {summary_dict['MAE_mean']:.4f} ± {summary_dict['MAE_std']:.4f}\\n\"\n",
    "        f\"  RUL Score (mean±std): {summary_dict['RUL_Score_mean']:.4f} ± {summary_dict['RUL_Score_std']:.4f}\\n\"\n",
    "        f\"  Folds:                {int(summary_dict['n_folds'])}\\n\"\n",
    "    )\n",
    "\n",
    "report_lines = []\n",
    "\n",
    "# Feature count + config header\n",
    "report_lines.append(f\"Dataset={DATASET_NAME} | Feature count={len(ALL_FEATURES)}\")\n",
    "report_lines.append(f\"Purged Group Time Series Split (PGTS): Window(W)={W}, Horizon(H)={H}, Embargo={EMBARGO}, Splits={N_SPLITS}\")\n",
    "\n",
    "# Holdout\n",
    "report_lines.append(\"\\n[Random 80/20 Split — XGBoost]\")\n",
    "report_lines.append(\n",
    "    f\"  R2: {hold_summary['R2']:.4f} | MSE: {hold_summary['MSE']:.4f} | MAE: {hold_summary['MAE']:.4f} | RUL Score: {hold_summary['RUL_Score']:.4f}\"\n",
    ")\n",
    "\n",
    "# PGTS main (opsiyonel olarak açılabilir)\n",
    "# report_lines.append(\"\\n[PGTS — XGBoost (Embargo=10)]\")\n",
    "# report_lines.append(fmt_summary(\"Macro summary\", pgts_e10_summary))\n",
    "\n",
    "# Null model (PGTS)\n",
    "report_lines.append(\"[PGTS — Null Model (label permutation) — XGBoost, Embargo=10]\")\n",
    "report_lines.append(fmt_summary(\"Results\", pgts_null_summary))\n",
    "\n",
    "# Embargo sensitivity\n",
    "report_lines.append(\"[PGTS — Embargo Sensitivity] — XGBoost (Embargo 0 vs 10)\")\n",
    "report_lines.append(fmt_summary(\"Embargo=0 Results\",  pgts_e0_summary))\n",
    "report_lines.append(fmt_summary(\"Embargo=10 Results\", pgts_e10_summary))\n",
    "\n",
    "final_report = \"\\n\".join(report_lines)\n",
    "print(final_report)\n",
    "save_txt(f\"FINAL_REPORT_{DATASET_NAME}_XGBoost.txt\", final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16977b9-d9ad-4da6-9106-17cc36893dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
