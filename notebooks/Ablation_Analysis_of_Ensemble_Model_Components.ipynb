{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90945ac1",
   "metadata": {},
   "source": [
    "\n",
    "# Ensemble Component Ablation\n",
    "\n",
    "This notebook evaluates four ensemble pairs on the chosen C-MAPSS subset using three scenarios per pair:\n",
    "- Single model (A)\n",
    "- Single model (B)\n",
    "- Fixed-weight averaging (70/30) — the higher-weighted side is chosen via cross-validation on the training split\n",
    "- Stacking (Ridge, α=1) — meta-learner fitted on out-of-fold predictions\n",
    "\n",
    "**Outputs**\n",
    "- A CSV summary with R², MSE, MAE, and execution time (seconds)\n",
    "- Bar charts for R² / MSE / MAE for each pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "except Exception:\n",
    "    LGBMRegressor = None\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except Exception:\n",
    "    CatBoostRegressor = None\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "except Exception:\n",
    "    XGBRegressor = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------\n",
    "# Configuration\n",
    "# -------------------\n",
    "DATA_DIR = \"data/CMAPSS\"  # location of C-MAPSS files\n",
    "DATASET = \"FD001\"         # FD001/FD002/FD003/FD004\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS_STACK = 5\n",
    "N_FOLDS_WEIGHT = 3\n",
    "\n",
    "OUT_DIR = \"ablation_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "INCLUDE_RUL_SCORE = True\n",
    "\n",
    "ENSEMBLE_PAIRS = [\n",
    "    (\"LGBM\", \"CAT\"),\n",
    "    (\"GBR\", \"XGB\"),\n",
    "    (\"RF\",  \"HGBR\"),\n",
    "    (\"MLP\", \"KNN\"),\n",
    "]\n",
    "\n",
    "PARAMS = {\n",
    "    \"LGBM\": dict(n_estimators=500, learning_rate=0.1, max_depth=-1, num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE),\n",
    "    \"CAT\":  dict(iterations=500, learning_rate=0.1, depth=6, random_seed=RANDOM_STATE, verbose=0),\n",
    "    \"GBR\":  dict(n_estimators=300, max_depth=7, learning_rate=0.1, subsample=0.8, random_state=RANDOM_STATE),\n",
    "    \"XGB\":  dict(objective=\"reg:squarederror\", learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=0),\n",
    "    \"RF\":   dict(n_estimators=500, random_state=RANDOM_STATE, n_jobs=0),\n",
    "    \"HGBR\": dict(random_state=RANDOM_STATE),\n",
    "    \"MLP\":  dict(hidden_layer_sizes=(50,50,50), activation=\"relu\", solver=\"adam\", alpha=0.01, learning_rate=\"constant\", max_iter=500, random_state=RANDOM_STATE),\n",
    "    \"KNN\":  dict(n_neighbors=14, weights=\"uniform\", metric=\"manhattan\"),\n",
    "}\n",
    "\n",
    "NEEDS_SCALING = {\"MLP\",\"KNN\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b769e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cmapss_train(dataset: str, data_dir: str) -> pd.DataFrame:\n",
    "    fname = os.path.join(data_dir, f\"train_{dataset}.txt\")\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"Missing file: {fname}\")\n",
    "    df = pd.read_csv(fname, sep=\"\\s+\", header=None)\n",
    "    cols = [\"unit\",\"cycle\"] + [f\"op{i}\" for i in range(1,4)] + [f\"s{i}\" for i in range(1,22)]\n",
    "    df = df.iloc[:, :len(cols)]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def add_rul_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    max_cycles = df.groupby(\"unit\")[\"cycle\"].max().rename(\"max_cycle\")\n",
    "    df = df.merge(max_cycles, on=\"unit\", how=\"left\")\n",
    "    df[\"RUL\"] = df[\"max_cycle\"] - df[\"cycle\"]\n",
    "    return df.drop(columns=[\"max_cycle\"])\n",
    "\n",
    "def train_test_group_split(df: pd.DataFrame, test_size: float, random_state: int):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    groups = df[\"unit\"].values\n",
    "    idx = np.arange(len(df))\n",
    "    tr, te = next(gss.split(idx, groups=groups))\n",
    "    return df.iloc[tr].copy(), df.iloc[te].copy()\n",
    "\n",
    "def rul_score(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    s = np.where(d < 0, np.exp(-d / 10.0), np.exp(d / 13.0))\n",
    "    return float(np.sum(s))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, include_rul=False):\n",
    "    out = {\"R2\": r2_score(y_true, y_pred),\n",
    "           \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "           \"MAE\": mean_absolute_error(y_true, y_pred)}\n",
    "    if include_rul:\n",
    "        out[\"RUL_Score\"] = rul_score(y_true, y_pred)\n",
    "    return out\n",
    "\n",
    "def feature_columns(df: pd.DataFrame):\n",
    "    return [c for c in df.columns if c not in (\"unit\",\"cycle\",\"RUL\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model(tag: str):\n",
    "    p = PARAMS[tag]\n",
    "    if tag == \"LGBM\":\n",
    "        if LGBMRegressor is None: raise ImportError(\"lightgbm not installed.\")\n",
    "        return LGBMRegressor(**p)\n",
    "    if tag == \"CAT\":\n",
    "        if CatBoostRegressor is None: raise ImportError(\"catboost not installed.\")\n",
    "        return CatBoostRegressor(**p)\n",
    "    if tag == \"GBR\": return GradientBoostingRegressor(**p)\n",
    "    if tag == \"XGB\":\n",
    "        if XGBRegressor is None: raise ImportError(\"xgboost not installed.\")\n",
    "        return XGBRegressor(**p)\n",
    "    if tag == \"RF\": return RandomForestRegressor(**p)\n",
    "    if tag == \"HGBR\": return HistGradientBoostingRegressor(**p)\n",
    "    if tag == \"MLP\": return MLPRegressor(**p)\n",
    "    if tag == \"KNN\": return KNeighborsRegressor(**p)\n",
    "    raise ValueError(tag)\n",
    "\n",
    "def build_pipeline(tag: str, numeric_features: List[str]):\n",
    "    model = make_model(tag)\n",
    "    if tag in NEEDS_SCALING:\n",
    "        pre = ColumnTransformer([(\"num\", StandardScaler(), numeric_features)], remainder=\"drop\")\n",
    "        return Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "    return Pipeline([(\"model\", model)])\n",
    "\n",
    "def timed_fit_predict(pipe: Pipeline, X_train, y_train, X_test):\n",
    "    t0 = time.perf_counter()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return y_pred, time.perf_counter() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10ca489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_weights_by_cv(tagA, tagB, X_train, y_train, groups, numeric_cols):\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS_WEIGHT)\n",
    "    mseA, mseB = [], []\n",
    "    for tr, va in gkf.split(X_train, y_train, groups):\n",
    "        X_tr, X_va = X_train.iloc[tr], X_train.iloc[va]\n",
    "        y_tr, y_va = y_train[tr], y_train[va]\n",
    "        pA = build_pipeline(tagA, numeric_cols); pB = build_pipeline(tagB, numeric_cols)\n",
    "        pA.fit(X_tr, y_tr); pB.fit(X_tr, y_tr)\n",
    "        mseA.append(mean_squared_error(y_va, pA.predict(X_va)))\n",
    "        mseB.append(mean_squared_error(y_va, pB.predict(X_va)))\n",
    "    return (0.7, 0.3) if float(np.mean(mseA)) <= float(np.mean(mseB)) else (0.3, 0.7)\n",
    "\n",
    "def stacking_predict(tagA, tagB, X_train, y_train, X_test, groups, numeric_cols):\n",
    "    t0 = time.perf_counter()\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS_STACK)\n",
    "    oof_A = np.zeros(len(X_train)); oof_B = np.zeros(len(X_train))\n",
    "    for tr, va in gkf.split(X_train, y_train, groups):\n",
    "        X_tr, X_va = X_train.iloc[tr], X_train.iloc[va]; y_tr = y_train[tr]\n",
    "        pA = build_pipeline(tagA, numeric_cols); pB = build_pipeline(tagB, numeric_cols)\n",
    "        pA.fit(X_tr, y_tr); pB.fit(X_tr, y_tr)\n",
    "        oof_A[va] = pA.predict(X_va); oof_B[va] = pB.predict(X_va)\n",
    "    meta = Ridge(alpha=1.0, random_state=RANDOM_STATE).fit(np.c_[oof_A,oof_B], y_train)\n",
    "    pA_full = build_pipeline(tagA, numeric_cols); pB_full = build_pipeline(tagB, numeric_cols)\n",
    "    pA_full.fit(X_train, y_train); pB_full.fit(X_train, y_train)\n",
    "    Z_test = np.c_[pA_full.predict(X_test), pB_full.predict(X_test)]\n",
    "    return meta.predict(Z_test), time.perf_counter() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138baded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class ScenarioResult:\n",
    "    pair: str\n",
    "    scenario: str\n",
    "    R2: float\n",
    "    MSE: float\n",
    "    MAE: float\n",
    "    ExecTimeSec: float\n",
    "    RUL_Score: float | None = None\n",
    "\n",
    "def run_ablation_for_pair(tagA, tagB, df_train):\n",
    "    df = add_rul_labels(df_train.copy())\n",
    "    X_cols = feature_columns(df)\n",
    "    df_tr, df_te = train_test_group_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    X_tr, X_te = df_tr[X_cols], df_te[X_cols]\n",
    "    y_tr, y_te = df_tr[\"RUL\"].values, df_te[\"RUL\"].values\n",
    "    groups_tr = df_tr[\"unit\"].values\n",
    "    numeric_cols = X_cols\n",
    "\n",
    "    results = []\n",
    "\n",
    "    pipeA = build_pipeline(tagA, numeric_cols)\n",
    "    predA, tA = timed_fit_predict(pipeA, X_tr, y_tr, X_te)\n",
    "    metA = compute_metrics(y_te, predA, include_rul=INCLUDE_RUL_SCORE)\n",
    "    results.append(ScenarioResult(pair=f\"{tagA}+{tagB}\", scenario=f\"{tagA} (Single)\", R2=metA['R2'], MSE=metA['MSE'], MAE=metA['MAE'], ExecTimeSec=tA, RUL_Score=metA.get('RUL_Score')))\n",
    "\n",
    "    pipeB = build_pipeline(tagB, numeric_cols)\n",
    "    predB, tB = timed_fit_predict(pipeB, X_tr, y_tr, X_te)\n",
    "    metB = compute_metrics(y_te, predB, include_rul=INCLUDE_RUL_SCORE)\n",
    "    results.append(ScenarioResult(pair=f\"{tagA}+{tagB}\", scenario=f\"{tagB} (Single)\", R2=metB['R2'], MSE=metB['MSE'], MAE=metB['MAE'], ExecTimeSec=tB, RUL_Score=metB.get('RUL_Score')))\n",
    "\n",
    "    wA, wB = choose_weights_by_cv(tagA, tagB, X_tr, y_tr, groups_tr, numeric_cols)\n",
    "    predW = wA*predA + wB*predB\n",
    "    tW = tA + tB\n",
    "    metW = compute_metrics(y_te, predW, include_rul=INCLUDE_RUL_SCORE)\n",
    "    results.append(ScenarioResult(pair=f\"{tagA}+{tagB}\", scenario=\"Weighted Average (7:3)\", R2=metW['R2'], MSE=metW['MSE'], MAE=metW['MAE'], ExecTimeSec=tW, RUL_Score=metW.get('RUL_Score')))\n",
    "\n",
    "    predS, tS = stacking_predict(tagA, tagB, X_tr, y_tr, X_te, groups_tr, numeric_cols)\n",
    "    metS = compute_metrics(y_te, predS, include_rul=INCLUDE_RUL_SCORE)\n",
    "    results.append(ScenarioResult(pair=f\"{tagA}+{tagB}\", scenario=\"Stacking\", R2=metS['R2'], MSE=metS['MSE'], MAE=metS['MAE'], ExecTimeSec=tS, RUL_Score=metS.get('RUL_Score')))\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_full_ablation(dataset=DATASET):\n",
    "    df_train = load_cmapss_train(dataset, DATA_DIR)\n",
    "    all_results = []\n",
    "    for tagA, tagB in ENSEMBLE_PAIRS:\n",
    "        all_results.extend(run_ablation_for_pair(tagA, tagB, df_train))\n",
    "    rows = []\n",
    "    for r in all_results:\n",
    "        row = dict(EnsemblePair=r.pair, Scenario=r.scenario, R2=r.R2, MSE=r.MSE, MAE=r.MAE, ExecTimeSec=r.ExecTimeSec)\n",
    "        if INCLUDE_RUL_SCORE: row[\"RUL_Score\"] = r.RUL_Score\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Ready. Call run_full_ablation() to compute results on the selected subset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6119bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Execute and save\n",
    "try:\n",
    "    results_df = run_full_ablation(DATASET)\n",
    "    display(results_df.head(20))\n",
    "    csv_path = os.path.join(OUT_DIR, f\"ablation_results_{DATASET}.csv\")\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(\"Saved:\", csv_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(str(e))\n",
    "    print(\"→ Check DATA_DIR and C-MAPSS files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metric_bars(df: pd.DataFrame, pair: str, metric: str):\n",
    "    subset = df[df[\"EnsemblePair\"] == pair]\n",
    "    labels = subset[\"Scenario\"].tolist()\n",
    "    values = subset[metric].tolist()\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(labels, values)\n",
    "    plt.title(f\"{pair} — {metric}\")\n",
    "    plt.ylabel(metric); plt.xlabel(\"Scenario\")\n",
    "    plt.xticks(rotation=20); plt.tight_layout()\n",
    "    p = os.path.join(OUT_DIR, f\"{pair.replace('+','_')}_{metric}.png\")\n",
    "    plt.savefig(p, dpi=160); plt.show(); print(\"Saved:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49015207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'results_df' in globals():\n",
    "    for pair in sorted(results_df[\"EnsemblePair\"].unique()):\n",
    "        for metric in [\"R2\",\"MSE\",\"MAE\"]:\n",
    "            plot_metric_bars(results_df, pair, metric)\n",
    "else:\n",
    "    print(\"Run the ablation cell first to generate results.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
